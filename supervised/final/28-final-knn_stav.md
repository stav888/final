# KNN

האם האלגוריתם KNN פותר בעיית קלסיפיקציה, רגרסיה או גם וגם?

**תשובה:**

ה KNN יכול לפתור גם קלסיפיקציה וגם רגרסיה. הוא אלגוריתם גמיש שמשמש בעיקר לסיווג, אבל עובד מצוין גם לחיזוי ערכים רציפים.

---

מה המשמעות של השם KNN?

**תשובה:**

ה KNN = K Nearest Neighbors, כלומר “K השכנים הקרובים ביותר”. הרעיון הוא להסתכל על K (מספר) הדגימות הכי קרובות לנקודה החדשה.

---

מהו הרעיון המרכזי שעליו מבוסס האלגוריתם KNN?

**תשובה:**

נקודות הדומות זו לזו במרחב הפיצ'רים - כלומר מרוחקות זו מזו מעט - בדרך כלל מקבלות תוצאה זהה. כדי לנבא ערך עבור נקודה חדשה מזהים את השכנים הקרובים לה ומסתמכים עליהם לקבלת ההחלטה.

---

מהו התפקיד של הפרמטר K באלגוריתם?

**תשובה**: K קובע כמה שכנים קרובים נכניס לחישוב הניבוי. לדוגמה: אם K=3, מסתכלים על 3 הנקודות הקרובות ביותר לנקודה החדשה.

---

האם K נחשב Hyperparameter? הסבר/י

**תשובה:**

כן, K הוא Hyperparameter כי אנחנו בוחרים אותו מראש (לפני הריצה), והוא לא נלמד מהדאטה כמו משקולות/מקדמים. שינוי של K משנה ישירות את התנהגות המודל ואת התוצאות.

---

כיצד בחירת ערך קטן של K משפיעה על המודל?

**תשובה:**

ערך K נמוך (למשל 1 או 3) גורם לכך שהמודל יהיה גמיש בצורה יתרה ויקלוט כל שינוי קטן בנתונים, הוא מתאים מאוד לנתוני האימון, אבל רגיש לרעש ויכול לגרום ל-overfitting.

---

כיצד בחירת ערך גדול של K משפיעה על המודל?

**תשובה:**

ערך K גדול הופך את המודל לחלק יותר ויציב – הוא מתעלם מרעש ומפרטים קטנים, אם הקפיצה ל-K גבוהה מדי, המודל ממוצע עד כדי כך שהוא מפספס את הדפוסים האמיתיים ונופל ל-underfitting.

---

בכדי למדוד את ה- K האידיאלי נשתמש בגרף ה ELBOW  
הסבר מה זה גרף ELBOW? מה בד"כ יש בציר ה- X וציר ה- Y?  
כיצד נבחר את ה- Sweet Spot?  
תן דוגמא לעוד מודל ששם נשתמש בגרף זה והסבר כיצד?  

**תשובה:**

גרף המרפק (Elbow) מראה איך ביצועי המודל משתנים כשמשנים את ערך הפרמטר. 
בתחילה יש שיפור גדול, אחר כך השיפור קטן והגרף נראה כמו מרפק. 

בציר ה-X מציבים את ערכי הפרמטר (למשל, ב-KNN זה K).  
בציר ה-Y מציבים את מדד הביצועים או השגיאה (למשל, MSE או Accuracy). 

איך מוצאים את הנקודה האידיאלית?  
בוחרים את הנקודה שבה השיפור כמעט נפסק — זה ה-"מרפק".  

דוגמה נוספת: 
ב-K-Means משתמשים בגרף כדי לקבוע את מספר הקבוצות K.  
בציר ה-X — מספר הקבוצות, בציר ה-Y — סכום ריבועי המרחקים בתוך הקבוצות. 
הנקודה שבה הירידה בגרף מאטה היא הנקודה האידיאלית. 

---

מה ההבדל בין KNN לקלסיפיקציה לבין KNN לרגרסיה?

**תשובה:**

ההבדל הוא באופן חישוב התוצאה הסופית מהשכנים. בקלסיפיקציה מקבלים את הסיווג השכיח ביותר, ברגרסיה מחשבים ממוצע של הערכים או ממוצע משוקלל.

---

כיצד מתקבלת התחזית ב־KNN לקלסיפיקציה?

**תשובה:**
1. מוצאים את K השכנים הקרובים לנקודה החדשה.
2. בודקים לאיזו מחלקה רוב השכנים שייכים.
3. מסווגים את הנקודה החדשה למחלקה הזו.
---

כיצד מתקבלת התחזית ב־KNN לרגרסיה?

**תשובה:**
1. מוצאים את K הנקודות שממוקמות הכי קרוב לנקודה החדשה.
2. מחשבים את הממוצע של הערכים Y של הנקודות הללו.
3. התוצאה היא הערך הממוצע.
---

איזו מדידת מרחק נפוצה משמשת ב־KNN?

**תשובה:**

הנפוצה ביותר היא מרחק אוקלידי - השורש הריבועי של סכום ההפרשים בריבוע בכל פיצ'ר.

---

כיצד מספר הפיצ'רים משפיע על ביצועי KNN?

**תשובה:**

כאשר מוסיפים פיצ'רים, המרחב הופך צפוף. המרחקים בין הנקודות גדלים, וקשה למצוא שכנים באמת קרובים. הרעש והנתונים שאינם רלוונטיים מטשטשים את התמונה, והדיוק יורד. 

כאשר מסירים פיצ'רים, המרחקים נחשבים מהר. אם הפיצ'רים שנשארו אינן מכסות את ההיבטים החשובים של הבעיה, המידע החיוני חומק והחיזוי נפגם.

---

האם קיים שלב אימון (Training) מובהק באלגוריתם KNN?

**תשובה:**

ב-KNN אין שלב אימון מובהק.  
ה-fit רק שומר את הנתונים בזיכרון.  
כל העבודה האמיתית קוראת רק בשלב הניבוי, כשמודדים מרחקים ומוצאים את ה-K שכנים הקרובים.  

---

כיצד Python פותר את חישוב KNN – האם באמצעות נוסחה סגורה או באמצעות חישוב ישיר בזמן החיזוי?

**תשובה:**
בPython ספריית sklearn לא משתמש בנוסחה סגורה ל-KNN.  
ב-KNN אין משוואה לפתרון, כל החישוב מתבצע ברגע החיזוי.  
המודל שומר את כל הדאטה בשלב האימון.  
בעת החיזוי הוא מחשב מרחק בין הדוגם החדש לכל דוגם באימון, בוחר את K השכנים הקרובים ומחליט על התווית. 

---

מהם היתרונות המרכזיים של KNN?

**תשובה:**

- פשוט וקל להבנה.
- מתאים גם לקלסיפיקציה וגם לרגרסיה.
- לא מניח צורה לינארית או פרמטרית מסוימת של הנתונים.
- אפשר לעדכן בקלות – פשוט מוסיפים דוגמאות חדשות לזיכרון בלי צורך באימון מחדש.
- עובד היטב כשהנתונים מקובצים מקומית ושכנים באמת דומים.

---

מהם החסרונות המרכזיים של KNN?

**תשובה:**

- חישוב יקר בזמן חיזוי – צריך למדוד מרחקים לכל (או לרוב) נקודות האימון.  
- רגיש לבחירת K – ערך לא מתאים עלול לגרום ל-overfitting או underfitting.  
- סובל מקללת המימדיות – כשיש הרבה פיצ'רים, המרחקים מאבדים משמעות.  
- רגיש לסקיילינג – פיצ'ר עם טווח גדול עלול לשלוט על חישוב המרחק אם לא מנרמלים.  
- אין מודל פרמטרי – קשה להפיק תובנות גלובליות (כמו משקולות) מהמודל.
---

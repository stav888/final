# KNN

האם האלגוריתם KNN פותר בעיית קלסיפיקציה, רגרסיה או גם וגם?

**תשובה:**

ה KNN יכול לפתור גם קלסיפיקציה וגם רגרסיה. הוא אלגוריתם גמיש שמשמש בעיקר לסיווג, אבל עובד מצוין גם לחיזוי ערכים רציפים.

---

מה המשמעות של השם KNN?

**תשובה:**

ה KNN = K Nearest Neighbors, כלומר “K השכנים הקרובים ביותר”. הרעיון הוא להסתכל על K (מספר) הדגימות הכי קרובות לנקודה החדשה.

---

מהו הרעיון המרכזי שעליו מבוסס האלגוריתם KNN?

**תשובה:**

נקודות הדומות זו לזו במרחב הפיצ'רים - כלומר מרוחקות זו מזו מעט - בדרך כלל מקבלות תוצאה זהה. כדי לנבא ערך עבור נקודה חדשה מזהים את השכנים הקרובים לה ומסתמכים עליהם לקבלת ההחלטה.

---

מהו התפקיד של הפרמטר K באלגוריתם?

**תשובה**: K קובע כמה שכנים קרובים נכניס לחישוב הניבוי. לדוגמה: אם K=3, מסתכלים על 3 הנקודות הקרובות ביותר לנקודה החדשה.

---

האם K נחשב Hyperparameter? הסבר/י

**תשובה:**

כן, K הוא Hyperparameter כי אנחנו בוחרים אותו מראש (לפני הריצה), והוא לא נלמד מהדאטה כמו משקולות/מקדמים. שינוי של K משנה ישירות את התנהגות המודל ואת התוצאות.

---

כיצד בחירת ערך קטן של K משפיעה על המודל?

**תשובה:**

ערך K נמוך (למשל 1 או 3) גורם לכך שהמודל יהיה גמיש בצורה יתרה ויקלוט כל שינוי קטן בנתונים, הוא מתאים מאוד לנתוני האימון, אבל רגיש לרעש ויכול לגרום ל-overfitting.

---

כיצד בחירת ערך גדול של K משפיעה על המודל?

**תשובה:**

ערך K גדול הופך את המודל לחלק יותר ויציב – הוא מתעלם מרעש ומפרטים קטנים, אם הקפיצה ל-K גבוהה מדי, המודל ממוצע עד כדי כך שהוא מפספס את הדפוסים האמיתיים ונופל ל-underfitting.

---

בכדי למדוד את ה- K האידיאלי נשתמש בגרף ה ELBOW  
הסבר מה זה גרף ELBOW? מה בד"כ יש בציר ה- X וציר ה- Y?  
כיצד נבחר את ה- Sweet Spot?  
תן דוגמא לעוד מודל ששם נשתמש בגרף זה והסבר כיצד?  

**תשובה:**

גרף המרפק (Elbow) מראה איך ביצועי המודל משתנים כשמשנים את ערך הפרמטר.<BR>
בתחילה יש שיפור גדול, אחר כך השיפור קטן והגרף נראה כמו מרפק.<BR>

בציר ה-X מציבים את ערכי הפרמטר (למשל, ב-KNN זה K).<BR>
בציר ה-Y מציבים את מדד הביצועים או השגיאה (למשל, MSE או Accuracy).<BR>

איך מוצאים את הנקודה האידיאלית?<BR>
בוחרים את הנקודה שבה השיפור כמעט נפסק — זה ה-"מרפק".<BR>

דוגמה נוספת:<BR>
ב-K-Means משתמשים בגרף כדי לקבוע את מספר הקבוצות K .<BR>
בציר ה-X — מספר הקבוצות, בציר ה-Y — סכום ריבועי המרחקים בתוך הקבוצות.<BR>
הנקודה שבה הירידה בגרף מאטה היא הנקודה האידיאלית.<BR>

---

מה ההבדל בין KNN לקלסיפיקציה לבין KNN לרגרסיה?

**תשובה:**

ההבדל הוא באופן חישוב התוצאה הסופית מהשכנים. בקלסיפיקציה מקבלים את הסיווג השכיח ביותר, ברגרסיה מחשבים ממוצע של הערכים או ממוצע משוקלל.

---

כיצד מתקבלת התחזית ב־KNN לקלסיפיקציה?

**תשובה:**
1. מוצאים את K השכנים הקרובים לנקודה החדשה.
2. בודקים לאיזו מחלקה רוב השכנים שייכים.
3. מסווגים את הנקודה החדשה למחלקה הזו.
---

כיצד מתקבלת התחזית ב־KNN לרגרסיה?

**תשובה:**
1. מוצאים את K הנקודות שממוקמות הכי קרוב לנקודה החדשה.
2. מחשבים את הממוצע של הערכים Y של הנקודות הללו.
3. התוצאה היא הערך הממוצע.
---

איזו מדידת מרחק נפוצה משמשת ב־KNN?

**תשובה:**

הנפוצה ביותר היא מרחק אוקלידי - השורש הריבועי של סכום ההפרשים בריבוע בכל פיצ'ר.

---

כיצד מספר הפיצ'רים משפיע על ביצועי KNN?

**תשובה:**

כאשר מוסיפים פיצ'רים, המרחב הופך צפוף. המרחקים בין הנקודות גדלים, וקשה למצוא שכנים באמת קרובים. הרעש והנתונים שאינם רלוונטיים מטשטשים את התמונה, והדיוק יורד. <BR>

כאשר מסירים פיצ'רים, המרחקים נחשבים מהר. אם הפיצ'רים שנשארו אינן מכסות את ההיבטים החשובים של הבעיה, המידע החיוני חומק והחיזוי נפגם.

---

האם קיים שלב אימון (Training) מובהק באלגוריתם KNN?

**תשובה:**

ב-KNN אין שלב אימון מובהק.<BR>
ה-fit רק שומר את הנתונים בזיכרון.<BR>
כל העבודה האמיתית קוראת רק בשלב הניבוי, כשמודדים מרחקים ומוצאים את ה-K שכנים הקרובים.<BR>

---

כיצד Python פותר את חישוב KNN – האם באמצעות נוסחה סגורה או באמצעות חישוב ישיר בזמן החיזוי?

**תשובה:**

---

מהם היתרונות המרכזיים של KNN?

**תשובה:**

---

מהם החסרונות המרכזיים של KNN?

**תשובה:**

---
